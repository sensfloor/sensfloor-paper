\section{Methods}
\label{sec:methods}
% General overview of how to solve the problem (localization and pose prediction)
The final goal of this project is to design a system that reads SensFloor signals, processes them, and visualizes the person's gait within a virtual simulation.
To achieve this goal, we split up the problem into two components: pose estimation and localization. 
The process of both components is illustrated in figure \ref{fig:methods-overview}.
By separating these tasks, the system is able to use a small localized active area of the floor for pose estimation. 
Afterward this pose is mapped across the global floor coordinates. This enables the system to be ported to various floors of different dimensions.

For pose estimation component, we implemented a supervised fine-tuning pipeline. For that we collected reference pose estimates from a MediaPipe to serve as labels.
Using these labels, we trained a Long Short-Term Memory (LSTM) to predict human poses based on SensFloor activations within an extracted Region of Interest (ROI).
To localize the estimated pose, a Kalman Filter processes the noisy sensor signals to determine the person's global coordinates.
Finally, the combined data of pose and position is streamed to a frontend application for real-time visualization.


\cite{bowman:reasoning} % File only compiles when at least one ref is provided
\begin{figure}[htbp]
    \centering
    \includegraphics{img/methods-overview.pdf}
    \caption{TEST TEST}
    \Description{Test :D}
    \label{fig:methods-overview}
\end{figure}

% Data collection (important: mention how our data looks like)
% - Who did we collect?
% - How does the recording setup look like?
% - Collect video and sensfloor at the same time -> Output: mp4 and csv
% - Creating labels for training using Mediapipe -> Output: Ground truth joint positions in coordinate system with hip as origin
\subsection{Data collection}
The model developed in this paper takes SensFloor signals as input to predict the 3D joint coordinates of a person walking on the floor.
To train and evaluate such a model, we recorded in two sessions a total of eight \todo{Look up actual duration} hours of synchronized SensFloor signals and video from three participants.
To synchronize the data, we mapped the SensFloor signals to the corresponding video frame number at their time of occurrence.
In post-processing, we used MediaPipe \todo{cite} to extract 3D pose estimates from the video to serve as training targets.
It is important to mention here that the origin of MediaPipe's pose estimates coordinate system is located at the center of the hips.
This allows us to decompose the paper's problem into the previously mentioned two components of pose estimation relative to the hip and localization on the whole floor.


% Describe training of the model
% - Model architecture: Supervised regression model
% - Data (roi, history)
% - Data split
% - Loss (link loss, MSE)
% - Training setup (config) and how we determined the hyperparameters -> Mention what joints we predict here (or in data collection)
% - Metrics (PCK, MJPE)
% - What is the output?
\subsection{Training Setup}
We used a CNN-LSTM architecture for training our model. It is based on the model from \todo{cite correctly}.
Instead of rotating the step, we use a CNN to account for the Translation invariance.
Figure STH shows the full architecture. 
Instead of giving the model all signals of the floor, we construct a Region of Interest (ROI)
which is a part of the floor containing the most signals in a defined sequence.
A sequence is a collection of frames. For each frame we have a state of the floor with certain signals.
Our model receives the signals in the ROI frame by frame and produces a vector with continuous values as logits.
These logits are given into our Loss function which can be mathematically described in the following
\todo{replace with correct equation}
We substract the predictions from the reference labels, square and multiply the weights of the landmarks.
The link loss is identical to the one in \todo{replace with correct cite}
We assume an upright position, this is why the upper body and the head will only rotate and the focus is more on the knee and feet
Mediapipe extracts 33 Joints, we only used 13, excluding the fingers, face expression, heels and foot index.
For each epoch we calculate the following metrics: Mean joint position error (MJPE), percentage correct keypoits (PCK) with a 5 and 10 threshold


We assume only one person is walking on the floor

% Describe localization
% - Sensor & clustering of signals
% - Kalman filter approach
% - Component values
% - What is the output?

% Describe how localization and pose prediction are combined -> Summary that connects training and localization outputs

% How to visualize floor and predicted pose
% - Setup for live predictions