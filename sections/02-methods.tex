\section{Methods}
\label{sec:methods}
% General overview of how to solve the problem (localization and pose prediction)
The final goal of this project is to design a system that reads SensFloor signals, processes them, and visualizes the person's gait within a virtual simulation.
To achieve this goal, we split up the problem into two components: pose estimation and localization. 
The process of both components is illustrated in figure \ref{fig:methods-overview}.
By separating these tasks, the system is able to use a small localized active area of the floor for pose estimation. 
Afterward this pose is mapped across the global floor coordinates. This enables the system to be ported to various floors of different dimensions.

For the pose estimation component, we implemented a supervised fine-tuning pipeline. For that we collected reference pose estimates from a MediaPipe to serve as labels.
Using these labels, we trained a Long Short-Term Memory (LSTM) to predict human poses based on SensFloor activations within an extracted Region of Interest (ROI).
To localize the estimated pose, a Kalman Filter processes the noisy sensor signals to determine the person's global coordinates.
Finally, the combined data of pose and position is streamed to a frontend application for real-time visualization.


\cite{bowman:reasoning} % File only compiles when at least one ref is provided
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/methods-overview.pdf}
    \caption{Overview of the SensFloor-based pose estimation pipeline --- The left branch estimates the pose in a local, position-independent coordinate system, while the right branch localizes the person in global floor coordinates.}
    \Description{}
    \label{fig:methods-overview}
\end{figure}

% Data collection (important: mention how our data looks like)
% - Who did we collect?
% - How does the recording setup look like?
% - Collect video and sensfloor at the same time -> Output: mp4 and csv
% - Creating labels for training using Mediapipe -> Output: Ground truth joint positions in coordinate system with hip as origin
\subsection{Data collection}
The model developed in this paper takes SensFloor signals as input to predict the 3D joint coordinates of a person walking on the floor.
To train and evaluate such a model, we recorded in two sessions a total of eight \todo{Look up actual duration} hours of synchronized SensFloor signals and video from three participants.
To synchronize the data, we mapped the SensFloor signals to the corresponding video frame number at their time of occurrence.
In post-processing, we used MediaPipe \todo{cite} to extract 3D pose estimates from the video to serve as training targets.
It is important to mention here that the origin of MediaPipe's pose estimates coordinate system is located at the center of the hips.
This allows us to decompose the paper's problem into the previously mentioned two components of pose estimation relative to the hip and localization on the whole floor.


% Describe training of the model
% - Model architecture: Supervised regression model
% - Data (roi, history)
% - Data split
% - Loss (link loss, MSE)
% - Training setup (config) and how we determined the hyperparameters -> Mention what joints we predict here (or in data collection)
% - Metrics (PCK, MJPE)
% - What is the output?
\subsection{Training Setup}
We used a CNN-LSTM architecture for training our model. It is based on the model from \todo{cite correctly}.
Instead of rotating the step, we use a CNN to account for the Translation invariance.
Figure STH shows the full architecture. 
Instead of giving the model all signals of the floor, we construct a Region of Interest (ROI)
which is a part of the floor containing the most signals in a defined sequence.
A sequence is a collection of frames. For each frame we have a state of the floor with certain signals.
Our model receives the signals in the ROI frame by frame and produces a vector with continuous values as logits.
These logits are given into our Loss function which can be mathematically described in the following
\todo{replace with correct equation}
We substract the predictions from the reference labels, square and multiply the weights of the landmarks.
The link loss is identical to the one in \todo{replace with correct cite}
We assume an upright position, this is why the upper body and the head will only rotate and the focus is more on the knee and feet
Mediapipe extracts 33 Joints, we only used 13, excluding the fingers, face expression, heels and foot index.
For each epoch we calculate the following metrics: Mean joint position error (MJPE), percentage correct keypoits (PCK) with a 5 and 10 threshold


We assume only one person is walking on the floor

% Describe localization
% - Sensor & clustering of signals
% - Kalman filter approach
% - Component values
% - What is the output?
\subsection{Localization}
To integrate the local pose estimate from the model into the global coordinate system of the floor, we developed a pipeline that tracks the subject's hip position relative to the SensFloor. We do this by extracting a raw pose estimate through clustering the measured sensor activations and applying a Kalman filter to smoothen the trajectory.

We identify the subject's contact points with the floor by grouping adjacent active sensor fields into clusters. The hip position is then calculated the following way: during a step, when both feet touch the ground, two clusters can be measured. In this case, we define the hip position as the midpoint between the centroids of the two clusters. If only one cluster is detected, meaning one foot is off the ground or both feet are close together, we assume the hip is located directly above that single centroid. This logic is illustrated in Figure \ref{fig:signal-clustering} and allows us to extract the hip position from the received signals.

\todo{Is this diagram valuable enough for the space?}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\columnwidth]{img/signal-clustering.pdf}
    \caption{Calculation of the global position --- This visualization shows two clusters of SensFloor activations. For both the centroid calculated. The final position is then determined by taking the mean position between both clusters.}
    \Description{}
    \label{fig:signal-clustering}
\end{figure}

Because the raw position estimates are noisy and lead to jittery, unnatural movement in the visualization, we apply a Kalman Filter to ensure a smooth trajectory. Similar to \cite[255 ff]{}, the filter's state vector is defined as $\mathbf{x} = [x, y, \dot{x}, \dot{y}]^T$, where $x$ and $y$ are the position coordinates and $\dot{x}$ and $\dot{y}$ the corresponding velocities. Due to the absence of ground-truth tracking data for exact calibration, we tuned the filter's noise parameters empirically until the trajectory visually matched the subject's actual movement in the recorded videos. A comparison of the Kalman-filtered position estimates versus the raw positions is shown in Figure \ref{fig:kalman-filter-effect}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/kalman-filter-effect.pdf}
    \caption{Effect of the Kalman filter --- The left part of the image shows the raw signals that we obtain from the clustering of the activated fields from the walk of a person on the floor. On the right side are the signals processed by the Kalman filter which show a much smoother and realistic trajectory.}
    \Description{}
    \label{fig:kalman-filter-effect}
\end{figure}


% Describe how localization and pose prediction are combined -> Summary that connects training and localization outputs

% How to visualize floor and predicted pose
% - Setup for live predictions