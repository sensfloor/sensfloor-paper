\section{Methods}
\label{sec:methods}
% General overview of how to solve the problem (localization and pose prediction)
To achieve this goal of estimating poses using a SensFloor, we split up the problem into two tasks, that are illustrated in Figure \ref{fig:methods-overview}: pose estimation and localization.
\todo{Rephrase}
Pose estimation estimates the person's joint coordinates in a local coordinate system.
For the pose estimation component, we implemented a supervised fine-tuning pipeline. For that we collected reference pose estimates from a MediaPipe to serve as targets.
Using these targets, we trained a Long Short-Term Memory (LSTM) to predict human poses based on SensFloor activations within an extracted Region of Interest (ROI).

To position the local pose estimate in the global coordinate system of the floor we extract positions using a clustering approach and smooth the resulting positions using a Kalman filter.
Finally, the combined data of pose and position is streamed to a frontend application for real-time visualization.

By separating these tasks, the system is able to use a small localized active area of the floor for pose estimation. 
Afterward this pose is mapped across the global floor coordinates. This enables the system to be ported to various floors of different dimensions.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/methods-overview.pdf}
    \caption{Overview of the SensFloor-based pose estimation pipeline --- The left branch estimates the pose in a local, position-independent coordinate system, while the right branch localizes the person in global floor coordinates.}
    \Description{}
    \label{fig:methods-overview}
\end{figure}

% Data collection (important: mention how our data looks like)
% - Who did we collect?
% - How does the recording setup look like?
% - Collect video and sensfloor at the same time -> Output: mp4 and csv
% - Creating targets for training using MediaPipe -> Output: Ground truth joint positions in coordinate system with hip as origin
\subsection{Data collection}
\todo{find better transition}
The model developed in this paper takes SensFloor signals as input to predict the 3D joint coordinates of a person walking on the floor.
To train and evaluate such a model, we recorded in two sessions a total of eight \todo{Look up actual duration} hours of synchronized SensFloor signals and video at 15~FPS from three participants.
In addition to this, there are further short recordings of three other people that we used for testing.
To synchronize the data, we mapped the SensFloor signals to the corresponding video frame number at their time of occurrence. 
In post-processing, we used MediaPipe \todo{cite} to extract 3D pose estimates from the video to serve as training targets.
It is important to mention here that the origin of MediaPipe's pose estimates coordinate system is located at the center of the hips.
This allows us to decompose the paper's problem into the previously mentioned two components of pose estimation relative to the hip and localization on the whole floor.

\subsection{Training Data preprocessing}
% - Unique frames
Before giving the data to our model we preprocess it in several ways. First we filter signals that are beneath a threshold and likely correspond to noise from the floor. We defined the threshold to be $\tau = 140$, but it could vary depending on the floor setup and its surface material. Then we select the frames, at which a significant SensFloor signal was measured and assign them the pose of that frame, ending up with a dataset containing both significant signals as input and a pose as the target.
% - Data split
Next, we split the dataset sequence-wise with $80\%$ for training, $10\%$ for validating and $10\%$ for testing. When we load one item of this set we create a ROI history, which is a temporal sequence of areas of highest activation. We defined the length of the history as 50 and the ROI is a $4\times4$ grid of the floor with the signal values for this frame. \
\todo{explain roi gird not triangular and use image and create better image} % maybe that's not so important but if we have space
% - Data (roi, history)
The history is created by going back 50 frames and for each frame taking the signals it received or no signals if no signals were in this frame.
% - normalize: 0-1 and not -1 to 1 because most will be 0. e.g. 127
All signals are normalized to the range $[0, 1]$, where values of 127 will be 0 and values of 255 will be 1.
% - Transformation (rotate)
Because the ROI is independent in the rest of the floor, we randomly rotate full ROI histories by $\{0, 90, 180, 270\}$ degrees for a more robust training.
% - Joints excluded
Finally, we use 13 of the 33 joints given by MediaPipe, excluding eyes, ears, mouth and fingers because they are irrelevant for our goal. Additionally, we exclude the foot heel and foot index, because when analyzing the data we found that MediaPipe predicted these bones unreliably to lengths of 3 to 18~cm.

\begin{figure}
    \centering
    \includegraphics[scale=0.75]{img/ROI-visualization.png}
    \caption{Example of one ROI in the sequence given to the model. The highlighted rectangle is the area of the ROI and the signals of the floor are in blue, showing the step of a person}
    \Description{}
    \label{fig:roi}
\end{figure}

\subsection{Pose estimation model}
% Describe training of the model
% - Model architecture: Supervised regression model
% - Loss (link loss, MSE)
% - Training setup (config) and how we determined the hyperparameters -> Mention what joints we predict here (or in data collection)
% - Metrics (PCK, MJPE)
% - What is the output?
\todo{Mention regression model}
As our model we chose an LSTM architecture. It is based on the model from \cite{hoffmann2021detectingwalkingchallenges}. We adapted in a way that we added a CNN layer before the LSTM and increased the model size. The model's full architecture and parameters are illustrated in figure \ref{fig:model-architecture}.  
\todo{Mention Epoch and such?} % google, find out if it's a standard to do this and also if annex is ok

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/model-architecture.pdf}
    \caption{Overview of the model. We used a CNN-LSTM architecture with a self attention layer}
    \Description{description model architecture}
    \label{fig:model-architecture}
\end{figure}

The CNN-LSTM model receives the signals in the ROI frame by frame and produces a vector of continuous values, representing the $x$, $y$ and $z$ coordinate predictions for each joint. These coordinates are given into the loss function. The total loss is defined as
\begin{equation}
    L_{total} = L_{\text{WMSE}} + \lambda \cdot L_{\text{link}},
    \label{eq:total_loss}
\end{equation}
where $\lambda$ is a fixed scalar for weighting $L_{\text{link}}$. For our training we set $\lambda=0.1$. The $L_{\text{WMSE}}$ (Weighted Mean Squared Error) calculates the squared difference between predictions and the targets, the joint coordinates, weighted by specific landmark importance and is defined as

\begin{equation}
L_{\text{WMSE}} = \frac{1}{B \cdot J \cdot 3} \sum_{i=1}^{B} \sum_{j=1}^{J}\sum_{k=1}^{3} w_j (\hat{y}_{i,j,k} - y_{i,j,k})^2,
\end{equation} 

where $B$ is the batch size, $J$ the number of joints, $k$ indexing the $x$, $y$ and $z$ coordinates of each joint, $\hat{y}_{i,j,k}$ the model's predictions and $y_{i,j,k}$ the targets. We increased the weight for the knees and feet to 5 compared to 1 for all other joints, because they are the most active components in a walking scenario. For each estimated joint-pair we furthermore calculate the Euclidean distance $d$ (e.g. the lower leg length). We use it in the link loss function which we adopted from \cite{luo2021intelligentcarpetinferring} defined as 

\begin{equation}
L_{\text{link}}(d) =
\begin{cases}
k_{\text{min}} - d, & \text{if } d < k_{\text{min}} \\
d - k_{\text{max}}, & \text{if } d > k_{\text{max}} \\
0, & \text{otherwise},
\end{cases}
\end{equation} 

where $k_{\text{min}}$ and $k_{\text{max}}$ are the $3^{\text{th}}$ and $97^{\text{th}}$ percentiles of the link lengths in the training set.
During the training we calculate the following metrics in each epoch: Mean joint position error (MJPE) and percentage correct keypoits (PCK) with a 5 and 10 threshold


% Describe localization
% - Sensor & clustering of signals
% - Kalman filter approach
% - Component values
% - What is the output?
\subsection{Localization}
To integrate the local pose estimate from the model into the global coordinate system of the floor, we developed a pipeline that tracks the subject's hip position relative to the SensFloor. We do this by extracting a raw pose estimate through clustering the measured sensor activations and applying a Kalman filter to smoothen the trajectory.

We identify the subject's contact points with the floor by grouping adjacent active sensor fields into clusters. The hip position is then calculated the following way: during a step, when both feet touch the ground, two clusters can be measured. In this case, we define the hip position as the midpoint between the centroids of the two clusters. If only one cluster is detected, meaning one foot is off the ground or both feet are close together, we assume the hip is located directly above that single centroid. This logic is illustrated in Figure \ref{fig:signal-clustering} and allows us to extract the hip position from the received signals.

\todo{Make horizontal}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\columnwidth]{img/signal-clustering.pdf}
    \caption{Calculation of the global position --- This visualization shows two clusters of SensFloor activations. For both the centroid calculated. The final position is then determined by taking the mean position between both clusters.}
    \Description{}
    \label{fig:signal-clustering}
\end{figure}

Because the raw position estimates are noisy and lead to erratic, unnatural movement in the visualization, we apply a Kalman Filter to ensure a smooth trajectory. Similar to \cite[255 ff.]{rogerr.2024kalmanbayesianfilters}, the filter's state vector is defined as $\mathbf{x} = [x, y, \dot{x}, \dot{y}]^T$, where $x$ and $y$ are the position coordinates and $\dot{x}$ and $\dot{y}$ the corresponding velocities. Due to the absence of ground-truth tracking data for exact calibration, we tuned the filter's noise parameters empirically until the trajectory visually matched the subject's actual movement in the recorded videos. A comparison of the Kalman-filtered position estimates versus the raw measured positions is shown in Figure \ref{fig:kalman-filter-effect}

\todo{move maybe to results}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/kalman-filter-effect.pdf}
    \caption{Effect of the Kalman filter --- The left part of the image shows the raw signals that we obtain from the clustering of the activated fields from the walk of a person on the floor. On the right side are the signals processed by the Kalman filter which show a much smoother and realistic trajectory.}
    \Description{}
    \label{fig:kalman-filter-effect}
\end{figure}


\subsection{Visualization of predicted 3D Pose}
For visual evaluating and demo-purposes of the pose estimation pipeline, we furthermore implemented a frontend. The estimated pose and its position are streamed to the frontend using a WebSocket. The frontend then visualizes a simulated environment of the floor and the person's pose moving on it.