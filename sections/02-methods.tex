\section{Methods}
\label{sec:methods}
% General overview of how to solve the problem (localization and pose prediction)
The final goal of this project is to design a system that reads SensFloor signals, processes them, and visualizes the person's gait within a virtual simulation.
To achieve this goal, we split up the problem into two components: pose estimation and localization. 
The process of both components is illustrated in figure \ref{fig:methods-overview}.
By separating these tasks, the system is able to use a small localized active area of the floor for pose estimation. 
Afterward this pose is mapped across the global floor coordinates. This enables the system to be ported to various floors of different dimensions.

For the pose estimation component, we implemented a supervised fine-tuning pipeline. For that we collected reference pose estimates from a MediaPipe to serve as labels.
Using these labels, we trained a Long Short-Term Memory (LSTM) to predict human poses based on SensFloor activations within an extracted Region of Interest (ROI).
To localize the estimated pose, a Kalman Filter processes the noisy sensor signals to determine the person's global coordinates.
Finally, the combined data of pose and position is streamed to a frontend application for real-time visualization.


\cite{bowman:reasoning} % File only compiles when at least one ref is provided
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/methods-overview.pdf}
    \caption{Overview of the SensFloor-based pose estimation pipeline --- The left branch estimates the pose in a local, position-independent coordinate system, while the right branch localizes the person in global floor coordinates.}
    \Description{}
    \label{fig:methods-overview}
\end{figure}

% Data collection (important: mention how our data looks like)
% - Who did we collect?
% - How does the recording setup look like?
% - Collect video and sensfloor at the same time -> Output: mp4 and csv
% - Creating labels for training using MediaPipe -> Output: Ground truth joint positions in coordinate system with hip as origin
\subsection{Data collection}
The model developed in this paper takes SensFloor signals as input to predict the 3D joint coordinates of a person walking on the floor.
To train and evaluate such a model, we recorded in two sessions a total of eight \todo{Look up actual duration} hours of synchronized SensFloor signals and video from three participants. \todo{mention 15fps? 10Hz?}
To synchronize the data, we mapped the SensFloor signals to the corresponding video frame number at their time of occurrence. 
In post-processing, we used MediaPipe \todo{cite} to extract 3D pose estimates from the video to serve as training targets. \todo{training targets or reference labels?}
It is important to mention here that the origin of MediaPipe's pose estimates coordinate system is located at the center of the hips.
This allows us to decompose the paper's problem into the previously mentioned two components of pose estimation relative to the hip and localization on the whole floor.


\subsection{Training Data preprocessing}
% - Unique frames
Before giving the data to our model we preprocess it in several ways. First we filter signals that are beneath a threshold and likely correspond to noise from the floor. We defined the threshold to be $\tau = 140$, but it can vary by the floor setup. \todo{equation format?}. Then we filter labels that do not contain SensFloor signals and vice versa ending up with a set of frames that contain both significant signals as input and a reference label. 
% - Data split
Next, we take the first $80\%$ of this list as the training data, $10\%$ as validation and $10\%$ as test. When we load one item of this set we create a ROI history, which is a sequence of areas of the floor. We defined the sequence length as 50 and the ROI is a $4\times4$ grid of the floor with the signal values for this frame.
\todo{Patch signals should be explained} % The patches send signals when the capacitance changes, so even if there are no signals for a frame, there is still a state
% - Data (roi, history)
The history is craeted by going back 50 frames and for each frame taking the signals it received or no signals if no signals were in this frame.
\todo{explain roi gird not triangular} % maybe that's not so important but if we have space
% - normalize: 0-1 and not -1 to 1 because most will be 0. e.g. 127
All signals are normalized to the range $[0, 1]$ (127 will be 0 and 255 will be 1).
% - Transformation (rotate)
Given the translation invariance of the data, we rotate the ROI history by ${0, 90, 180, 270}$ randomly for a more robust training.
% - Joints excluded
Finally we use 13 of the 33 joints given by MediaPipe, excluding eyes, ears, mouth and fingers because they are irrevelant for our goal. Additionally we exclude the Foot heel and index, because MediaPipe predicted the foot unreliably to lengths of 3~cm to 18~cm.

\subsection{Training Setup}
% Describe training of the model
% - Model architecture: Supervised regression model
% - Loss (link loss, MSE)
% - Training setup (config) and how we determined the hyperparameters -> Mention what joints we predict here (or in data collection)
% - Metrics (PCK, MJPE)
% - What is the output?
We used a LSTM architecture for training our model. It is based on the model from \cite{hoffmann_gait2021}. We instead use a CNN to account for the Translation invariance. The full architecture is in figure \ref{fig:model-architecture}. 
Instead of giving the model all signals of the floor, we construct a Region of Interest (ROI) which is a part of the floor containing the most signals in a defined sequence. A sequence is a collection of frames. For each frame we have a state of the floor. The state is constantly updated by the signals received from the patches. The CNN-LSTM model receives the signals in the ROI frame by frame and produces a vector with continuous values, the coordinate predictions for each joint. These coordinates are given into the Loss function. The total loss is defined as

\begin{equation}
    L_{total} = L_{\text{WMSE}} + \lambda \cdot L_{\text{link}},
    \label{eq:total_loss}
\end{equation}
where $\lambda$ is a fixed scalar for $L_{\text{link}}$. We set $\lambda$ to $0.1$. The $L_{\text{WMSE}}$ calculates the squared difference between predictions and the reference labels, the joint coordinates, weighted by specific landmark importance and is defined as

\begin{equation}
L_{\text{WMSE}} = \frac{1}{B \cdot J \cdot 3} \sum_{i=1}^{B} \sum_{j=1}^{J}\sum_{k=1}^{3} w_j (\hat{y}_{i,j,k} - y_{i,j,k})^2,
\end{equation} 

where $B$ is the batch size, $J$ the number of joints, $k$ x,y and z coordinates of each joint, $\hat{y}_{i,j,k}$ the model's predictions and $y_{i,j,k}$ the reference labels. 
Furthermore we used the link loss from \cite{Luo_intel2021} defined as 

\begin{equation}
L_{\text{link}}(d) =
\begin{cases}
k_{\text{min}} - d, & \text{if } d < k_{\text{min}} \\
d - k_{\text{max}}, & \text{if } d > k_{\text{max}} \\
0, & \text{otherwise},
\end{cases}
\end{equation} 

where $d$ is the length of a link (e.g. shoulder width) and $k_{\text{min}}$ and $k_{\text{max}}$ are the top and bottom $0.03$ percentiles of the link lengths in the training set.

\todo{expl. MediaPipe inconsistency?}
% like where do we explain that we excluded the feet joints that were very inconsistent in length



We assume an upright position, this is why the upper body and the head will only rotate and the focus is more on the knee and feet
MediaPipe extracts 33 Joints, we only used 13, excluding the fingers, face expression, heels and foot index.
For each epoch we calculate the following metrics: Mean joint position error (MJPE), percentage correct keypoits (PCK) with a 5 and 10 threshold

\begin{figure}[htbp]
    \centering
    \includegraphics{model-architecture.pdf}
    \caption{caption model architecture}
    \Description{description model architecture}
    \label{fig:model-architecture}
\end{figure}


We assume only one person is walking on the floor

% Describe localization
% - Sensor & clustering of signals
% - Kalman filter approach
% - Component values
% - What is the output?
\subsection{Localization}
To integrate the local pose estimate from the model into the global coordinate system of the floor, we developed a pipeline that tracks the subject's hip position relative to the SensFloor. We do this by extracting a raw pose estimate through clustering the measured sensor activations and applying a Kalman filter to smoothen the trajectory.

We identify the subject's contact points with the floor by grouping adjacent active sensor fields into clusters. The hip position is then calculated the following way: during a step, when both feet touch the ground, two clusters can be measured. In this case, we define the hip position as the midpoint between the centroids of the two clusters. If only one cluster is detected, meaning one foot is off the ground or both feet are close together, we assume the hip is located directly above that single centroid. This logic is illustrated in Figure \ref{fig:signal-clustering} and allows us to extract the hip position from the received signals.

\todo{Is this diagram valuable enough for the space?}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\columnwidth]{img/signal-clustering.pdf}
    \caption{Calculation of the global position --- This visualization shows two clusters of SensFloor activations. For both the centroid calculated. The final position is then determined by taking the mean position between both clusters.}
    \Description{}
    \label{fig:signal-clustering}
\end{figure}

Because the raw position estimates are noisy and lead to jittery, unnatural movement in the visualization, we apply a Kalman Filter to ensure a smooth trajectory. Similar to \cite[255 ff]{}, the filter's state vector is defined as $\mathbf{x} = [x, y, \dot{x}, \dot{y}]^T$, where $x$ and $y$ are the position coordinates and $\dot{x}$ and $\dot{y}$ the corresponding velocities. Due to the absence of ground-truth tracking data for exact calibration, we tuned the filter's noise parameters empirically until the trajectory visually matched the subject's actual movement in the recorded videos. A comparison of the Kalman-filtered position estimates versus the raw positions is shown in Figure \ref{fig:kalman-filter-effect}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/kalman-filter-effect.pdf}
    \caption{Effect of the Kalman filter --- The left part of the image shows the raw signals that we obtain from the clustering of the activated fields from the walk of a person on the floor. On the right side are the signals processed by the Kalman filter which show a much smoother and realistic trajectory.}
    \Description{}
    \label{fig:kalman-filter-effect}
\end{figure}


% Describe how localization and pose prediction are combined -> Summary that connects training and localization outputs

% How to visualize floor and predicted pose
% - Setup for live predictions