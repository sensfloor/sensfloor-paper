\section{Methods}
\label{sec:methods}
% General overview of how to solve the problem (localization and pose prediction)
To achieve the goal of estimating poses using a SensFloor and visualizing them, we split up the problem into two tasks, that are illustrated in Figure \ref{fig:methods-overview}: pose estimation and localization.

The first task of pose estimation uses a trained deep-learning model that estimates the person's joint coordinates in a local coordinate system. For that we collected a labeled dataset of SensFloor signals and target poses. Using this dataset, we trained a Long Short-Term Memory (LSTM) model to predict human poses from sensor activations within an extracted Region of Interest (ROI).

The second task of localization extracts the global position of the pose on the complete floor by signal clustering and smoothing the resulting position using a Kalman filter.

Finally, the estimated pose and position are combined and visualized in a real-time simulation of the SensFloor environment.

By splitting the problem into these two tasks, the system requires only a small extracted active area of the floor for pose estimation. This enables the system to be ported to other floors of different dimensions.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/methods-overview.pdf}
    \caption{Overview of the SensFloor-based pose estimation pipeline --- The left branch estimates the person's pose in a local, position-independent coordinate system, while the right branch localizes the person globally on the floor.}
    \Description{}
    \label{fig:methods-overview}
\end{figure}

% Data collection (important: mention how our data looks like)
% - Who did we collect?
% - How does the recording setup look like?
% - Collect video and sensfloor at the same time -> Output: mp4 and csv
% - Creating targets for training using MediaPipe -> Output: Ground truth joint positions in coordinate system with hip as origin
\subsection{Data collection}
To enable supervised training of the LSTM model, we collected a dataset of SensFloor signals and corresponding pose targets. 
For this purpose, we recorded in two sessions a total of eight hours of synchronized SensFloor signals and video at 15~FPS of three participants walking on the floor. Furthermore, we captured some additional short sequences from three further participants that are used for testing only. To synchronize the data, we matched each received SensFloor signal to the video frame number recorded at the same time. To extract the final 3D pose targets from the video feed, we used a MediaPipe pose estimation model.  \todo{cite} It is worth noting that the origin of MediaPipe's pose estimates coordinate system is located at the center of the hips. This allows the decomposition of the paper's problem into the previously introduced two tasks of pose estimation relative to the hip and localization on the global floor.

\subsection{Training Data preprocessing}
% - Unique frames
Before giving the data to our model we preprocess it in several ways. First we filter signals that are beneath a threshold and likely correspond to noise from the floor. We defined the threshold to be $\tau = 140$, but it could vary depending on the floor setup and its surface material. Then we select the frames, at which a significant SensFloor signal was measured and assign them the pose of that frame, ending up with a dataset containing both significant signals as input and a pose as the target.
% - Data split
Next, we split the dataset sequence-wise with $80\%$ for training, $10\%$ for validating and $10\%$ for testing. When we load one item of this set we create a ROI history, which is a temporal sequence of areas of highest activation. We defined the length of the history as 50 and the ROI is a $4\times4$ grid of the floor with the signal values for this frame. \
\todo{explain roi gird not triangular and use image and create better image} % maybe that's not so important but if we have space
% - Data (roi, history)
The history is created by going back 50 frames and for each frame taking the signals it received or no signals if no signals were in this frame.
% - normalize: 0-1 and not -1 to 1 because most will be 0. e.g. 127
All signals are normalized to the range $[0, 1]$, where values of 127 will be 0 and values of 255 will be 1.
% - Transformation (rotate)
Because the ROI is independent in the rest of the floor, we randomly rotate full ROI histories by $\{0, 90, 180, 270\}$ degrees for a more robust training.
% - Joints excluded
Finally, we use 13 of the 33 joints given by MediaPipe, excluding eyes, ears, mouth and fingers because they are irrelevant for our goal. Additionally, we exclude the foot heel and foot index, because when analyzing the data we found that MediaPipe predicted these bones unreliably to lengths of 3 to 18~cm.

\begin{figure}
    \centering
    \includegraphics[scale=0.75]{img/ROI-visualization.png}
    \caption{Example of one ROI in the sequence given to the model. The highlighted rectangle is the area of the ROI and the signals of the floor are in blue, showing the step of a person}
    \Description{}
    \label{fig:roi}
\end{figure}

\subsection{Pose estimation model}
% Describe training of the model
% - Model architecture: Supervised regression model
% - Loss (link loss, MSE)
% - Training setup (config) and how we determined the hyperparameters -> Mention what joints we predict here (or in data collection)
% - Metrics (PCK, MJPE)
% - What is the output?
\todo{Mention regression model}
As our model we chose an LSTM architecture. It is based on the model from \cite{hoffmann2021detectingwalkingchallenges}. We adapted in a way that we added a CNN layer before the LSTM and increased the model size. The model's full architecture and parameters are illustrated in figure \ref{fig:model-architecture}.  
\todo{Mention Epoch and such?} % google, find out if it's a standard to do this and also if annex is ok

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/model-architecture.pdf}
    \caption{Overview of the model. We used a CNN-LSTM architecture with a self attention layer}
    \Description{description model architecture}
    \label{fig:model-architecture}
\end{figure}

The CNN-LSTM model receives the signals in the ROI frame by frame and produces a vector of continuous values, representing the $x$, $y$ and $z$ coordinate predictions for each joint. These coordinates are given into the loss function. The total loss is defined as
\begin{equation}
    L_{total} = L_{\text{WMSE}} + \lambda \cdot L_{\text{link}},
    \label{eq:total_loss}
\end{equation}
where $\lambda$ is a fixed scalar for weighting $L_{\text{link}}$. For our training we set $\lambda=0.1$. The $L_{\text{WMSE}}$ (Weighted Mean Squared Error) calculates the squared difference between predictions and the targets, the joint coordinates, weighted by specific landmark importance and is defined as

\begin{equation}
L_{\text{WMSE}} = \frac{1}{B \cdot J \cdot 3} \sum_{i=1}^{B} \sum_{j=1}^{J}\sum_{k=1}^{3} w_j (\hat{y}_{i,j,k} - y_{i,j,k})^2,
\end{equation} 

where $B$ is the batch size, $J$ the number of joints, $k$ indexing the $x$, $y$ and $z$ coordinates of each joint, $\hat{y}_{i,j,k}$ the model's predictions and $y_{i,j,k}$ the targets. We increased the weight for the knees and feet to 5 compared to 1 for all other joints, because they are the most active components in a walking scenario. For each estimated joint-pair we furthermore calculate the Euclidean distance $d$ (e.g. the lower leg length). We use it in the link loss function which we adopted from \cite{luo2021intelligentcarpetinferring} defined as 

\begin{equation}
L_{\text{link}}(d) =
\begin{cases}
k_{\text{min}} - d, & \text{if } d < k_{\text{min}} \\
d - k_{\text{max}}, & \text{if } d > k_{\text{max}} \\
0, & \text{otherwise},
\end{cases}
\end{equation} 

where $k_{\text{min}}$ and $k_{\text{max}}$ are the $3^{\text{th}}$ and $97^{\text{th}}$ percentiles of the link lengths in the training set.
During the training we calculate the following metrics in each epoch: Mean joint position error (MJPE) and percentage correct keypoits (PCK) with a 5 and 10 threshold


% Describe localization
% - Sensor & clustering of signals
% - Kalman filter approach
% - Component values
% - What is the output?
\subsection{Localization}
\label{subsec:localization}
To integrate the local pose estimate from the model into the global coordinate system of the floor, we developed a pipeline that tracks the subject's hip position relative to the SensFloor. We do this by extracting a raw pose estimate through clustering the measured sensor activations and applying a Kalman filter to smoothen the trajectory.

We identify the subject's contact points with the floor by grouping adjacent active sensor fields into clusters. The hip position is then calculated the following way: during a step, when both feet touch the ground, two clusters can be measured. In this case, we define the hip position as the midpoint between the centroids of the two clusters. If only one cluster is detected, meaning one foot is off the ground or both feet are close together, we assume the hip is located directly above that single centroid. This logic is illustrated in Figure \ref{fig:signal-clustering} and allows us to extract the hip position from the received signals.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\columnwidth]{img/signal-clustering.pdf}
    \caption{Global position estimation from signals --- Two clusters of SensFloor activations are identified and their centroids (black dots) are computed. The final position is obtained as the midpoint between the two centroids (red cross).}
    \Description{}
    \label{fig:signal-clustering}
\end{figure}

Because the raw position estimates are noisy and lead to erratic, unnatural movement in the visualization, we apply a Kalman Filter to ensure a smooth trajectory. Similar to \cite[255 ff.]{rogerr.2024kalmanbayesianfilters}, the filter's state vector is defined as $\mathbf{x} = [x, y, \dot{x}, \dot{y}]^T$, where $x$ and $y$ are the position coordinates and $\dot{x}$ and $\dot{y}$ the corresponding velocities. Due to the absence of ground-truth tracking data for exact calibration, we tuned the filter's noise parameters empirically until the trajectory visually matched the subject's actual movement in the recorded videos.


\subsection{Visualization of predicted 3D Pose}
For visual evaluating and demo-purposes of the pose estimation pipeline, we furthermore implemented a frontend. The estimated pose and its position are streamed to the frontend using a WebSocket. The frontend then visualizes a simulated environment of the floor and the person's pose moving on it.